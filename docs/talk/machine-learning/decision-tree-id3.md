---
title: 决策树 - id3 算法
---

决策树是一种机器学习的方法。决策树的生成算法有 ID3, C4.5 和 C5.0 等。

**决策树 就是用熵对数据 分段 然后 层层条件 判断**

- [AiLearning](https://github.com/apachecn/AiLearning)
- [决策树算法之 ID3 与 C4.5 的理解与实现](https://www.cnblogs.com/lliuye/p/9008901.html)
- [加推实战之全栈 JS 经典机器学习](https://mp.weixin.qq.com/s/2q7z0PjqiXtBc1Gh1QsaaA)
- [JS 简单实现决策树(ID3 算法)](https://www.jianshu.com/p/2b50a98cd75c)

![](https://gitee.com/alvin0216/cdn/raw/master/img/algorithm/id3.png)

## 信息熵 & 信息增益

熵（entropy）: 熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。

信息论（information theory）中的熵（香农熵）: 是一种信息的度量方式，表示信息的混乱程度，也就是说: 信息越有序，信息熵越低。例如: 火柴有序放在火柴盒里，熵值很低，相反，熵值很高。

信息增益（information gain）: 在划分数据集前后信息发生的变化称为信息增益。

## 工作原理

```py
def createBranch():
'''
此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。
'''
    检测数据集中的所有数据的分类标签是否相同:
        If so return 类标签
        Else:
            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中
            return 分支节点
```

## 开发流程

```js
收集数据: 可以使用任何方法。
准备数据: 树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)
分析数据: 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
训练算法: 构造树的数据结构。
测试算法: 使用训练好的树计算错误率。
使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。
```

## 算法特点

```js
优点: 计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。
缺点: 容易过拟合。
适用数据类型: 数值型和标称型。
```

<!-- 基于 ID3 算法的决策树构建，其选择特征的准则是`信息增益`。信息增益表示得知特征 𝑋 的信息而使得类 𝑌 的信息的不确定性减少的程度。也就是说，信息增益越大，通过特征 𝑋 ，就越能够准确地将样本进行分类；信息增益越小，越无法准确进行分类。

在介绍信息增益之前，我们需要先对`熵`进行一下讲解。

### 熵

熵是度量样本集合纯度最常用的一种指标，它是信息的期望值。我们首先了解一下什么是信息。由《机器学习实战》中定义 -->
